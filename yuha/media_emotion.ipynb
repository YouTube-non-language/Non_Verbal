{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    0: ['Angry', (0,0,255), (255,255,255)],\n",
    "    1: ['Disgust', (0,102,0), (255,255,255)],\n",
    "    2: ['Fear', (255,255,153), (0,51,51)],\n",
    "    3: ['Happy', (153,0,153), (255,255,255)],\n",
    "    4: ['Sad', (255,0,0), (255,255,255)],\n",
    "    5: ['Surprise', (0,255,0), (255,255,255)],\n",
    "    6: ['Neutral', (160,160,160), (255,255,255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1./255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "        \n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                    loss=categorical_crossentropy,\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        self.checkpoint_path = checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_,h_max))\n",
    "    return image\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48,48))\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image):\n",
    "    H, W, _ = image.shape\n",
    "    \n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    if results.detections:\n",
    "        faces = []\n",
    "        pos = []\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            # mp_drawing.draw_detection(image, detection)\n",
    "\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2,x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            faces.append(face)\n",
    "            pos.append((x1, y1, x2, y2))\n",
    "    \n",
    "        x = recognition_preprocessing(faces)\n",
    "\n",
    "        y_1 = model_1.predict(x)\n",
    "        y_2 = model_2.predict(x)\n",
    "        l = np.argmax(y_1+y_2, axis=1)\n",
    "\n",
    "        for i in range(len(faces)):\n",
    "            cv2.rectangle(image, (pos[i][0],pos[i][1]),\n",
    "                            (pos[i][2],pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            cv2.rectangle(image, (pos[i][0],pos[i][1]-20),\n",
    "                            (pos[i][2]+20,pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0],pos[i][1]-5),\n",
    "                            0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'C:/Users/user/Downloads/lecture.mp4'\n",
    "cap = cv2.VideoCapture(video)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "target_h = 360\n",
    "target_w = int(target_h * frame_width / frame_height)\n",
    "out = cv2.VideoWriter('run/out.avi',cv2.VideoWriter_fourcc('M','J','P','G'),\n",
    "                      fps, (target_w,target_h))\n",
    "\n",
    "while True:\n",
    "    success, image = cap.read()\n",
    "    if success:\n",
    "        # image = resize_image(image)\n",
    "        result = inference(image)\n",
    "        out.write(result)\n",
    "        cv2.imshow('Emotion Detection', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def onTrackbarChange(pos, *userdata):\n",
    "    # Set the video capture position based on the trackbar position\n",
    "    cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", pos)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, pos)\n",
    "\n",
    "video_path = 'C:/Users/user/Downloads/lecture.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "target_h = 360\n",
    "target_w = int(target_h * frame_width / frame_height)\n",
    "\n",
    "out = cv2.VideoWriter('run/out.avi', cv2.VideoWriter_fourcc('M','J','P','G'), fps, (target_w, target_h))\n",
    "\n",
    "paused = False\n",
    "\n",
    "# Create a trackbar to control video playback\n",
    "cv2.namedWindow('Emotion Detection')\n",
    "cv2.createTrackbar(\"Position\", \"Emotion Detection\", 0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), onTrackbarChange)\n",
    "\n",
    "while True:\n",
    "    if not paused:\n",
    "        success, image = cap.read()\n",
    "        if success:\n",
    "            # Assuming 'inference' returns an image\n",
    "            result = inference(image)\n",
    "            \n",
    "            # Write the processed frame to the output video\n",
    "            out.write(result)\n",
    "            \n",
    "            # Display the original frame (or processed frame if needed)\n",
    "            cv2.imshow('Emotion Detection', image)\n",
    "            \n",
    "            # Update the trackbar position based on the current frame\n",
    "            cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", int(cap.get(cv2.CAP_PROP_POS_FRAMES)))\n",
    "            \n",
    "            # Check for 'p' key to pause/unpause the video\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('p'):\n",
    "                paused = not paused\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        # If paused, just display the last frame\n",
    "        cv2.imshow('Emotion Detection', image)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('p'):\n",
    "            paused = not paused\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "emotions = {\n",
    "    0: ['Angry', (0,0,255), (255,255,255)],\n",
    "    1: ['Disgust', (0,102,0), (255,255,255)],\n",
    "    2: ['Fear', (255,255,153), (0,51,51)],\n",
    "    3: ['Happy', (153,0,153), (255,255,255)],\n",
    "    4: ['Sad', (255,0,0), (255,255,255)],\n",
    "    5: ['Surprise', (0,255,0), (255,255,255)],\n",
    "    6: ['Neutral', (160,160,160), (255,255,255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'\n",
    "\n",
    "\n",
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1./255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "        \n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                    loss=categorical_crossentropy,\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_,h_max))\n",
    "    return image\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48,48))\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x\n",
    "\n",
    "def inference(image):\n",
    "    H, W, _ = image.shape\n",
    "    \n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    if results.detections:\n",
    "        faces = []\n",
    "        pos = []\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            # mp_drawing.draw_detection(image, detection)\n",
    "\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2,x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            faces.append(face)\n",
    "            pos.append((x1, y1, x2, y2))\n",
    "    \n",
    "        x = recognition_preprocessing(faces)\n",
    "\n",
    "        y_1 = model_1.predict(x)\n",
    "        y_2 = model_2.predict(x)\n",
    "        l = np.argmax(y_1+y_2, axis=1)\n",
    "\n",
    "        for i in range(len(faces)):\n",
    "            cv2.rectangle(image, (pos[i][0],pos[i][1]),\n",
    "                            (pos[i][2],pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            cv2.rectangle(image, (pos[i][0],pos[i][1]-20),\n",
    "                            (pos[i][2]+20,pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0],pos[i][1]-5),\n",
    "                            0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# Video 감정분석\n",
    "def onTrackbarChange(pos, *userdata):\n",
    "    # 트랙바 위치를 기준으로 비디오 캡처 위치 설정\n",
    "    cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", pos)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, pos)\n",
    "\n",
    "video_path = 'C:/Users/user/Downloads/lecture.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "target_h = 360\n",
    "target_w = int(target_h * frame_width / frame_height)\n",
    "\n",
    "out = cv2.VideoWriter('run/out.avi', cv2.VideoWriter_fourcc('M','J','P','G'), fps, (target_w, target_h))\n",
    "\n",
    "paused = False\n",
    "\n",
    "# 비디오 재생을 제어할 트랙바 생성\n",
    "cv2.namedWindow('Emotion Detection')\n",
    "cv2.createTrackbar(\"Position\", \"Emotion Detection\", 0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), onTrackbarChange)\n",
    "\n",
    "while True:\n",
    "    if not paused:\n",
    "        success, image = cap.read()\n",
    "        if success:\n",
    "            # 'inference' 이미지 가정 추정\n",
    "            result = inference(image)\n",
    "            \n",
    "            # 처리된 프레임을 출력 비디오에 기록\n",
    "            out.write(result)\n",
    "            \n",
    "            # 원 프레임 표시\n",
    "            cv2.imshow('Emotion Detection', image)\n",
    "            \n",
    "            # 현재 프레임을 기준으로 트랙바 위치 업데이트\n",
    "            cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", int(cap.get(cv2.CAP_PROP_POS_FRAMES)))\n",
    "            \n",
    "            # 비디오 중지 'p' 키\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('p'):\n",
    "                paused = not paused\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        # 일시 중지된 경우 마지막 프레임만 표시\n",
    "        cv2.imshow('Emotion Detection', image)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('p'):\n",
    "            paused = not paused\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "emotions = {\n",
    "    0: ['Angry', (0,0,255), (255,255,255)],\n",
    "    1: ['Disgust', (0,102,0), (255,255,255)],\n",
    "    2: ['Fear', (255,255,153), (0,51,51)],\n",
    "    3: ['Happy', (153,0,153), (255,255,255)],\n",
    "    4: ['Sad', (255,0,0), (255,255,255)],\n",
    "    5: ['Surprise', (0,255,0), (255,255,255)],\n",
    "    6: ['Neutral', (160,160,160), (255,255,255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'\n",
    "\n",
    "\n",
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1./255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "        \n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                    loss=categorical_crossentropy,\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_,h_max))\n",
    "    return image\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48,48))\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x\n",
    "\n",
    "def inference(image):\n",
    "    H, W, _ = image.shape\n",
    "    \n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    if results.detections:\n",
    "        faces = []\n",
    "        pos = []\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            # mp_drawing.draw_detection(image, detection)\n",
    "\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2,x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            faces.append(face)\n",
    "            pos.append((x1, y1, x2, y2))\n",
    "    \n",
    "        x = recognition_preprocessing(faces)\n",
    "\n",
    "        y_1 = model_1.predict(x)\n",
    "        y_2 = model_2.predict(x)\n",
    "        l = np.argmax(y_1+y_2, axis=1)\n",
    "\n",
    "        for i in range(len(faces)):\n",
    "            cv2.rectangle(image, (pos[i][0],pos[i][1]),\n",
    "                            (pos[i][2],pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            cv2.rectangle(image, (pos[i][0],pos[i][1]-20),\n",
    "                            (pos[i][2]+20,pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0],pos[i][1]-5),\n",
    "                            0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# Video 감정분석\n",
    "def onTrackbarChange(pos, *userdata):\n",
    "    # 트랙바 위치를 기준으로 비디오 캡처 위치 설정\n",
    "    cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", pos)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, pos)\n",
    "\n",
    "video_path = 'C:/Users/user/Downloads/lecture.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "target_h = 360\n",
    "target_w = int(target_h * frame_width / frame_height)\n",
    "\n",
    "out = cv2.VideoWriter('run/out.avi', cv2.VideoWriter_fourcc('M','J','P','G'), fps, (target_w, target_h))\n",
    "\n",
    "paused = False\n",
    "\n",
    "# 비디오 재생을 제어할 트랙바 생성\n",
    "cv2.namedWindow('Emotion Detection')\n",
    "cv2.createTrackbar(\"Position\", \"Emotion Detection\", 0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), onTrackbarChange)\n",
    "\n",
    "\n",
    "# 감정별 카운트 초기화\n",
    "emotion_counts = {emotion_label: 0 for emotion_label in emotions}\n",
    "\n",
    "\n",
    "while True:\n",
    "    if not paused:\n",
    "        success, image = cap.read()\n",
    "        if success:\n",
    "            # 'inference' 이미지 가정 추정\n",
    "            result = inference(image)\n",
    "\n",
    "            # 얼굴과 감정 분석\n",
    "            emotion_predictions = []  # 각 얼굴에 대한 감정 분석 결과를 리스트로 저장\n",
    "            faces = []  # 얼굴 이미지를 저장하는 리스트 추가\n",
    "            \n",
    "            for i in range(len(faces)):\n",
    "                x = recognition_preprocessing([faces[i]])\n",
    "                y_1 = model_1.predict(x)\n",
    "                y_2 = model_2.predict(x)\n",
    "                l = np.argmax(y_1 + y_2, axis=1)\n",
    "                emotion_predictions.append(l[0])\n",
    "\n",
    "                # 각 얼굴에 대한 감정 카운트 증가\n",
    "                emotion_counts[l[0]] += 1\n",
    "            \n",
    "            # 처리된 프레임을 출력 비디오에 기록\n",
    "            out.write(result)\n",
    "            \n",
    "            # 원 프레임 표시\n",
    "            cv2.imshow('Emotion Detection', image)\n",
    "            \n",
    "            # 현재 프레임을 기준으로 트랙바 위치 업데이트\n",
    "            cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", int(cap.get(cv2.CAP_PROP_POS_FRAMES)))\n",
    "            \n",
    "            # 비디오 중지 'p' 키\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('p'):\n",
    "                paused = not paused\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        # 일시 중지된 경우 마지막 프레임만 표시\n",
    "        cv2.imshow('Emotion Detection', image)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('p'):\n",
    "            paused = not paused\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"감정 분석 결과:\")\n",
    "for label, count in emotion_counts.items():\n",
    "    emotion_name = emotions[label][0]\n",
    "    print(f\"{emotion_name}: {count} 번\")\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: Layer count mismatch when loading weights from file. 에러 해결방법 - 모델 다시 저장\n",
    "model.save_weights('fer2013_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# 미디어파이프 초기화\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Face Detection 모델 초기화\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "# 감정 레이블\n",
    "emotions = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Sad',\n",
    "    5: 'Surprise',\n",
    "    6: 'Neutral'\n",
    "}\n",
    "\n",
    "# 감정 분석 함수\n",
    "def analyze_emotion(face_image):\n",
    "    # 여기에 감정 분석 모델을 적용하는 코드를 추가하세요.\n",
    "    # 예를 들어, TensorFlow를 사용하여 감정을 분석할 수 있습니다.\n",
    "\n",
    "    # 이 함수에서는 단순히 랜덤하게 감정을 선택하는 것으로 대체합니다.\n",
    "    return emotions[np.random.randint(0, len(emotions))]\n",
    "\n",
    "# 비디오 캡처 초기화\n",
    "cap = cv2.VideoCapture('C:/Users/user/Downloads/lecture.mp4')\n",
    "\n",
    "# 감정 결과 카운트 딕셔너리\n",
    "emotion_counts = {emotion: 0 for emotion in emotions.values()}\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # 미디어파이프 Face Detection 적용\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    # 감정 분석 결과 출력\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            ih, iw, _ = image.shape\n",
    "            bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                   int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "            \n",
    "            face_image = image[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
    "\n",
    "            # 감정 분석 결과\n",
    "            emotion_result = analyze_emotion(face_image)\n",
    "\n",
    "            # 결과 출력\n",
    "            cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f'Emotion: {emotion_result}', (bbox[0], bbox[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # 감정 결과 카운트 업데이트\n",
    "            emotion_counts[emotion_result] += 1\n",
    "\n",
    "    # 화면에 표시\n",
    "    cv2.imshow('Emotion Analysis', image)\n",
    "\n",
    "    # 종료 키 확인\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 종료 시 감정 결과 카운트 출력\n",
    "print(\"감정 결과 카운트:\")\n",
    "for emotion, count in emotion_counts.items():\n",
    "    print(f\"{emotion}: {count}\")\n",
    "\n",
    "# 종료\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 미디어파이프 초기화\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Face Detection 모델 초기화\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "# 감정 레이블\n",
    "emotions = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Sad',\n",
    "    5: 'Surprise',\n",
    "    6: 'Neutral'\n",
    "}\n",
    "\n",
    "# 감정 분석 모델\n",
    "def create_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(7, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# 모델 생성\n",
    "model = create_model((48, 48, 1))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 로드 (훈련된 가중치)\n",
    "model.load_weights('fer2013_model_weights.h5')\n",
    "\n",
    "# 비디오 캡처 초기화\n",
    "cap = cv2.VideoCapture('C:/Users/user/Downloads/lecture.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "     # 이미지 확인\n",
    "    if image is None or image.size == 0:\n",
    "        continue  # 이미지가 비어 있으면 다음 프레임으로 넘어갑니다.\n",
    "\n",
    "    # 이미지 차원 확인\n",
    "    if image.shape[0] == 0 or image.shape[1] == 0:\n",
    "        continue  # 이미지의 높이 또는 너비가 0이면 다음 프레임으로 넘어갑니다.\n",
    "\n",
    "    # 미디어파이프 Face Detection 적용\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    # 감정 분석 결과 출력\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            ih, iw, _ = image.shape\n",
    "            bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                   int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "            \n",
    "            face_image = cv2.cvtColor(image[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]], cv2.COLOR_BGR2GRAY)\n",
    "            face_image = cv2.resize(face_image, (48, 48))\n",
    "            face_image = np.expand_dims(face_image, axis=-1)\n",
    "            face_image = np.expand_dims(face_image, axis=0)\n",
    "\n",
    "            # 감정 예측\n",
    "            predictions = model.predict(face_image)\n",
    "            emotion_label = np.argmax(predictions)\n",
    "            emotion_result = emotions[emotion_label]\n",
    "\n",
    "            # 결과 출력\n",
    "            cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f'Emotion: {emotion_result}', (bbox[0], bbox[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # 화면에 표시\n",
    "    cv2.imshow('Emotion Analysis', image)\n",
    "\n",
    "    # 종료 키 확인\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 종료\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: Layer count mismatch when loading weights from file. 에러 해결방법 - 모델 다시 저장\n",
    "model.save_weights('fer2013_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "emotions = {\n",
    "    0: ['Angry', (0, 0, 255), (255, 255, 255)],\n",
    "    1: ['Disgust', (0, 102, 0), (255, 255, 255)],\n",
    "    2: ['Fear', (255, 255, 153), (0, 51, 51)],\n",
    "    3: ['Happy', (153, 0, 153), (255, 255, 255)],\n",
    "    4: ['Sad', (255, 0, 0), (255, 255, 255)],\n",
    "    5: ['Surprise', (0, 255, 0), (255, 255, 255)],\n",
    "    6: ['Neutral', (160, 160, 160), (255, 255, 255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'\n",
    "\n",
    "\n",
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1. / 255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "\n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                     loss=categorical_crossentropy,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_, h_max))\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48, 48))\n",
    "\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x\n",
    "\n",
    "\n",
    "def inference(image):\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    faces = []  # 빈 리스트로 초기화\n",
    "\n",
    "    if results.detections:\n",
    "        pos = []\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2, x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            faces.append(face)\n",
    "            pos.append((x1, y1, x2, y2))\n",
    "\n",
    "        x = recognition_preprocessing(faces)\n",
    "\n",
    "        y_1 = model_1.predict(x)\n",
    "        y_2 = model_2.predict(x)\n",
    "        l = np.argmax(y_1 + y_2, axis=1)\n",
    "\n",
    "        for i in range(len(faces)):\n",
    "            cv2.rectangle(image, (pos[i][0], pos[i][1]),\n",
    "                          (pos[i][2], pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.rectangle(image, (pos[i][0], pos[i][1] - 20),\n",
    "                          (pos[i][2] + 20, pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0], pos[i][1] - 5),\n",
    "                        0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image, faces\n",
    "\n",
    "\n",
    "# Video 감정분석\n",
    "def onTrackbarChange(pos, *userdata):\n",
    "    # 트랙바 위치를 기준으로 비디오 캡처 위치 설정\n",
    "    if pos >= 0:\n",
    "        cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", pos)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, pos)\n",
    "\n",
    "\n",
    "# 동영상 파일이 있는 폴더 경로\n",
    "video_folder_path = 'C:/Users/user/Downloads/test_media'\n",
    "\n",
    "# 폴더 내의 모든 동영상 파일 경로를 얻어옴\n",
    "video_paths = glob.glob(os.path.join(video_folder_path, '*.mp4'))\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    target_h = 360\n",
    "    target_w = int(target_h * frame_width / frame_height)\n",
    "\n",
    "    out = cv2.VideoWriter(f'run/out_{os.path.basename(video_path)}.avi', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),\n",
    "                          fps, (target_w, target_h))\n",
    "\n",
    "    paused = False\n",
    "\n",
    "    # 비디오 재생을 제어할 트랙바 생성\n",
    "    cv2.namedWindow('Emotion Detection')\n",
    "    cv2.createTrackbar(\"Position\", \"Emotion Detection\", 0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), onTrackbarChange)\n",
    "\n",
    "    # 감정별 카운트 초기화\n",
    "    emotion_counts = {emotion_label: 0 for emotion_label in emotions}\n",
    "\n",
    "    while True:\n",
    "        if not paused:\n",
    "            success, image = cap.read()\n",
    "            if success:\n",
    "                # 'inference' 이미지 가정 추정\n",
    "                result, faces = inference(image)  # 'inference' 함수에서 얼굴을 가져옵니다\n",
    "\n",
    "                # 얼굴과 감정 분석\n",
    "                emotion_predictions = []  # 각 얼굴에 대한 감정 분석 결과를 리스트로 저장\n",
    "\n",
    "                for i in range(len(faces)):\n",
    "                    x = recognition_preprocessing([faces[i]])\n",
    "                    y_1 = model_1.predict(x)\n",
    "                    y_2 = model_2.predict(x)\n",
    "                    l = np.argmax(y_1 + y_2, axis=1)\n",
    "                    emotion_predictions.append(l[0])\n",
    "\n",
    "                    # 각 얼굴에 대한 감정 카운트 증가\n",
    "                    emotion_counts[l[0]] += 1\n",
    "\n",
    "                # 처리된 프레임을 출력 비디오에 기록\n",
    "                out.write(result)\n",
    "\n",
    "                # 원 프레임 표시\n",
    "                cv2.imshow('Emotion Detection', image)\n",
    "\n",
    "                # 현재 프레임을 기준으로 트랙바 위치 업데이트\n",
    "                cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", int(cap.get(cv2.CAP_PROP_POS_FRAMES)))\n",
    "\n",
    "                # 비디오 중지 'p' 키\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('p'):\n",
    "                    paused = not paused\n",
    "                elif key == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            # 일시 중지된 경우 마지막 프레임만 표시\n",
    "            cv2.imshow('Emotion Detection', image)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('p'):\n",
    "                paused = not paused\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    # 감정 분석 결과를 그래프로 나타내기\n",
    "    emotion_names = [emotions[label][0] for label in emotion_counts.keys()]\n",
    "    emotion_values = list(emotion_counts.values())\n",
    "\n",
    "    # 감정 색상 값을 0에서 1로 정규화\n",
    "    colors = [(emotions[label][1][0] / 255, emotions[label][1][1] / 255, emotions[label][1][2] / 255) for label in\n",
    "              emotion_counts.keys()]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(emotion_names, emotion_values, color=colors)\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Emotion Analysis Results')\n",
    "    plt.show()\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # 감정 분석 결과를 저장할 딕셔너리 생성\n",
    "    emotion_results = {\n",
    "        \"emotion_counts\": emotion_counts,\n",
    "        \"emotion_names\": emotion_names,\n",
    "        \"emotion_values\": emotion_values\n",
    "    }\n",
    "\n",
    "    # 결과를 JSON 파일로 저장\n",
    "    json_file_path = f'emotion_results_{os.path.basename(video_path)}.json'\n",
    "    json_file_path = os.path.join(video_folder_path, json_file_path)\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(emotion_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Emotion analysis results for {os.path.basename(video_path)} saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정본 - 영상출력 되는 대신 깔끔\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "emotions = {\n",
    "    0: ['Angry', (0, 0, 255), (255, 255, 255)],\n",
    "    1: ['Disgust', (0, 102, 0), (255, 255, 255)],\n",
    "    2: ['Fear', (255, 255, 153), (0, 51, 51)],\n",
    "    3: ['Happy', (153, 0, 153), (255, 255, 255)],\n",
    "    4: ['Sad', (255, 0, 0), (255, 255, 255)],\n",
    "    5: ['Surprise', (0, 255, 0), (255, 255, 255)],\n",
    "    6: ['Neutral', (160, 160, 160), (255, 255, 255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'\n",
    "\n",
    "\n",
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1. / 255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "\n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                     loss=categorical_crossentropy,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_, h_max))\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48, 48))\n",
    "\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x\n",
    "\n",
    "\n",
    "def inference(image):\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    faces = []  # 빈 리스트로 초기화\n",
    "\n",
    "    if results.detections:\n",
    "        pos = []\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2, x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            faces.append(face)\n",
    "            pos.append((x1, y1, x2, y2))\n",
    "\n",
    "        x = recognition_preprocessing(faces)\n",
    "\n",
    "        y_1 = model_1.predict(x)\n",
    "        y_2 = model_2.predict(x)\n",
    "        l = np.argmax(y_1 + y_2, axis=1)\n",
    "\n",
    "        for i in range(len(faces)):\n",
    "            cv2.rectangle(image, (pos[i][0], pos[i][1]),\n",
    "                          (pos[i][2], pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.rectangle(image, (pos[i][0], pos[i][1] - 20),\n",
    "                          (pos[i][2] + 20, pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0], pos[i][1] - 5),\n",
    "                        0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image, faces\n",
    "\n",
    "\n",
    "# Video 감정분석\n",
    "def onTrackbarChange(pos, *userdata):\n",
    "    # 트랙바 위치를 기준으로 비디오 캡처 위치 설정\n",
    "    if pos >= 0:\n",
    "        cv2.setTrackbarPos(\"Position\", \"Emotion Detection\", pos)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, pos)\n",
    "\n",
    "# 모든 동영상 결과를 저장할 빈 딕셔너리 초기화\n",
    "combined_results = {}\n",
    "\n",
    "# 동영상 파일이 있는 폴더 경로\n",
    "video_folder_path = 'C:/Users/user/Downloads/test_media'\n",
    "\n",
    "# 폴더 내의 모든 동영상 파일 경로를 얻어옴\n",
    "video_paths = glob.glob(os.path.join(video_folder_path, '*.mp4'))\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # 감정별 카운트 초기화\n",
    "    emotion_counts = {emotion_label: 0 for emotion_label in emotions}\n",
    "\n",
    "    while True:\n",
    "        success, image = cap.read()\n",
    "        if success:\n",
    "            # 'inference' 이미지 가정 추정\n",
    "            result, faces = inference(image)  # 'inference' 함수에서 얼굴을 가져옵니다\n",
    "\n",
    "            # 얼굴과 감정 분석\n",
    "            for i in range(len(faces)):\n",
    "                x = recognition_preprocessing([faces[i]])\n",
    "                y_1 = model_1.predict(x)\n",
    "                y_2 = model_2.predict(x)\n",
    "                l = np.argmax(y_1 + y_2, axis=1)\n",
    "\n",
    "                # 각 얼굴에 대한 감정 카운트 증가\n",
    "                emotion_counts[l[0]] += 1\n",
    "\n",
    "            # 프레임 표시\n",
    "            cv2.imshow('Emotion Detection', result)\n",
    "\n",
    "            # 'q' 키를 누르면 종료\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # 각 동영상에 대한 감정 분석 결과를 저장할 딕셔너리 생성\n",
    "    emotion_results = {\n",
    "        \"emotion_counts\": emotion_counts,\n",
    "        \"emotion_names\": [emotions[label][0] for label in emotion_counts.keys()],\n",
    "        \"emotion_values\": list(emotion_counts.values())\n",
    "    }\n",
    "\n",
    "    # 개별 동영상 결과를 저장\n",
    "    json_file_path = f'emotion_results_{os.path.basename(video_path)}.json'\n",
    "    json_file_path = os.path.join(video_folder_path, json_file_path)\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(emotion_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"{os.path.basename(video_path)}의 감정 분석 결과가 {json_file_path}에 저장되었습니다.\")\n",
    "\n",
    "    # 통합된 딕셔너리에 결과 저장\n",
    "    combined_results[os.path.basename(video_path)] = emotion_results\n",
    "\n",
    "# 통합된 결과를 하나의 JSON 파일로 저장\n",
    "combined_json_file_path = os.path.join(video_folder_path, 'combined_emotion_results.json')\n",
    "with open(combined_json_file_path, 'w') as json_file:\n",
    "    json.dump(combined_results, json_file, indent=4)\n",
    "\n",
    "print(f\"통합된 감정 분석 결과가 {combined_json_file_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 출력 없이 결과값만 저장 / 3초에 1번씩 프레임 처리\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "emotions = {\n",
    "    0: ['Angry', (0, 0, 255), (255, 255, 255)],\n",
    "    1: ['Disgust', (0, 102, 0), (255, 255, 255)],\n",
    "    2: ['Fear', (255, 255, 153), (0, 51, 51)],\n",
    "    3: ['Happy', (153, 0, 153), (255, 255, 255)],\n",
    "    4: ['Sad', (255, 0, 0), (255, 255, 255)],\n",
    "    5: ['Surprise', (0, 255, 0), (255, 255, 255)],\n",
    "    6: ['Neutral', (160, 160, 160), (255, 255, 255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'\n",
    "\n",
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1. / 255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "\n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                     loss=categorical_crossentropy,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_, h_max))\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48, 48))\n",
    "\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x\n",
    "\n",
    "\n",
    "def inference(image):\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    faces = []  # 빈 리스트로 초기화\n",
    "\n",
    "    if results.detections:\n",
    "        pos = []\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2, x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            faces.append(face)\n",
    "            pos.append((x1, y1, x2, y2))\n",
    "\n",
    "        x = recognition_preprocessing(faces)\n",
    "\n",
    "        y_1 = model_1.predict(x)\n",
    "        y_2 = model_2.predict(x)\n",
    "        l = np.argmax(y_1 + y_2, axis=1)\n",
    "\n",
    "        for i in range(len(faces)):\n",
    "            cv2.rectangle(image, (pos[i][0], pos[i][1]),\n",
    "                          (pos[i][2], pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.rectangle(image, (pos[i][0], pos[i][1] - 20),\n",
    "                          (pos[i][2] + 20, pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0], pos[i][1] - 5),\n",
    "                        0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image, faces\n",
    "\n",
    "# Video 감정분석\n",
    "def save_emotion_results(video_path, emotion_counts):\n",
    "    # 각 동영상에 대한 감정 분석 결과를 저장할 딕셔너리 생성\n",
    "    emotion_results = {\n",
    "        \"emotion_counts\": emotion_counts,\n",
    "        \"emotion_names\": [emotions[label][0] for label in emotion_counts.keys()],\n",
    "        \"emotion_values\": list(emotion_counts.values())\n",
    "    }\n",
    "\n",
    "    # 개별 동영상 결과를 저장\n",
    "    json_file_path = f'emotion_results_{os.path.basename(video_path)}.json'\n",
    "    json_file_path = os.path.join(video_folder_path, json_file_path)\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(emotion_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"{os.path.basename(video_path)}의 감정 분석 결과가 {json_file_path}에 저장되었습니다.\")\n",
    "\n",
    "    return emotion_results\n",
    "\n",
    "\n",
    "# 모든 동영상 결과를 저장할 빈 딕셔너리 초기화\n",
    "combined_results = {}\n",
    "\n",
    "# 동영상 파일이 있는 폴더 경로\n",
    "video_folder_path = 'C:/Users/user/Downloads/test_media'\n",
    "\n",
    "# 폴더 내의 모든 동영상 파일 경로를 얻어옴\n",
    "video_paths = glob.glob(os.path.join(video_folder_path, '*.mp4'))\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Load the video duration for accurate interval processing\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    video_duration = total_frames / fps\n",
    "\n",
    "    # 감정별 카운트 초기화 (각 동영상당 한 번 초기화)\n",
    "    emotion_counts = {emotion_label: 0 for emotion_label in emotions}\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "\n",
    "        # 3초당 1번씩 처리 (간격을 더 짧게 조정하여 자주 감정 분석)\n",
    "        if elapsed_time >= 3.0:\n",
    "            # 'inference' 이미지 가정 추정\n",
    "            result, faces = inference(image)  # 'inference' 함수에서 얼굴을 가져옵니다\n",
    "\n",
    "            # 얼굴과 감정 분석\n",
    "            for i in range(len(faces)):\n",
    "                x = recognition_preprocessing([faces[i]])\n",
    "                y_1 = model_1.predict(x)\n",
    "                y_2 = model_2.predict(x)\n",
    "                l = np.argmax(y_1 + y_2, axis=1)\n",
    "\n",
    "                # 각 얼굴에 대한 감정 카운트 증가\n",
    "                emotion_counts[l[0]] += 1\n",
    "\n",
    "            # 초기화 및 최종 결과 저장\n",
    "            if elapsed_time >= 3.0:\n",
    "                save_emotion_results(video_path, emotion_counts)\n",
    "                start_time = time.time()\n",
    "                # 3초 간격으로 최종 결과만 저장하고 나면 더 이상 처리할 필요 없음\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # 통합된 딕셔너리에 결과 저장\n",
    "    combined_results[os.path.basename(video_path)] = emotion_counts\n",
    "\n",
    "# 통합된 결과를 하나의 JSON 파일로 저장\n",
    "combined_json_file_path = os.path.join(video_folder_path, 'combined_emotion_results.json')\n",
    "with open(combined_json_file_path, 'w') as json_file:\n",
    "    json.dump(combined_results, json_file, indent=4)\n",
    "\n",
    "print(f\"통합된 감정 분석 결과가 {combined_json_file_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 출력 없이 결과값만 저장\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "emotions = {\n",
    "    0: ['Angry', (0, 0, 255), (255, 255, 255)],\n",
    "    1: ['Disgust', (0, 102, 0), (255, 255, 255)],\n",
    "    2: ['Fear', (255, 255, 153), (0, 51, 51)],\n",
    "    3: ['Happy', (153, 0, 153), (255, 255, 255)],\n",
    "    4: ['Sad', (255, 0, 0), (255, 255, 255)],\n",
    "    5: ['Surprise', (0, 255, 0), (255, 255, 255)],\n",
    "    6: ['Neutral', (160, 160, 160), (255, 255, 255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'\n",
    "\n",
    "\n",
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1. / 255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "\n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                     loss=categorical_crossentropy,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_, h_max))\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48, 48))\n",
    "\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x\n",
    "\n",
    "\n",
    "def inference(image):\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    faces = []  # 빈 리스트로 초기화\n",
    "\n",
    "    if results.detections:\n",
    "        pos = []\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2, x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            faces.append(face)\n",
    "            pos.append((x1, y1, x2, y2))\n",
    "\n",
    "        x = recognition_preprocessing(faces)\n",
    "\n",
    "        y_1 = model_1.predict(x)\n",
    "        y_2 = model_2.predict(x)\n",
    "        l = np.argmax(y_1 + y_2, axis=1)\n",
    "\n",
    "        for i in range(len(faces)):\n",
    "            cv2.rectangle(image, (pos[i][0], pos[i][1]),\n",
    "                          (pos[i][2], pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.rectangle(image, (pos[i][0], pos[i][1] - 20),\n",
    "                          (pos[i][2] + 20, pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0], pos[i][1] - 5),\n",
    "                        0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image, faces\n",
    "\n",
    "\n",
    "# Video 감정분석\n",
    "# 이 부분 수정\n",
    "def save_emotion_results(video_path, emotion_counts):\n",
    "    # 각 동영상에 대한 감정 분석 결과를 저장할 딕셔너리 생성\n",
    "    emotion_results = {\n",
    "        \"emotion_counts\": emotion_counts,\n",
    "        \"emotion_names\": [emotions[label][0] for label in emotion_counts.keys()],\n",
    "        \"emotion_values\": list(emotion_counts.values())\n",
    "    }\n",
    "\n",
    "    # 개별 동영상 결과를 저장\n",
    "    json_file_path = f'emotion_results_{os.path.basename(video_path)}.json'\n",
    "    json_file_path = os.path.join(video_folder_path, json_file_path)\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(emotion_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"{os.path.basename(video_path)}의 감정 분석 결과가 {json_file_path}에 저장되었습니다.\")\n",
    "\n",
    "    return emotion_results\n",
    "\n",
    "\n",
    "# 모든 동영상 결과를 저장할 빈 딕셔너리 초기화\n",
    "combined_results = {}\n",
    "\n",
    "# 동영상 파일이 있는 폴더 경로\n",
    "video_folder_path = 'C:/Users/user/Downloads/test_media'\n",
    "\n",
    "# 폴더 내의 모든 동영상 파일 경로를 얻어옴\n",
    "video_paths = glob.glob(os.path.join(video_folder_path, '*.mp4'))\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # 감정별 카운트 초기화\n",
    "    emotion_counts = {emotion_label: 0 for emotion_label in emotions}\n",
    "\n",
    "    while True:\n",
    "        success, image = cap.read()\n",
    "        if success:\n",
    "            # 'inference' 이미지 가정 추정\n",
    "            result, faces = inference(image)  # 'inference' 함수에서 얼굴을 가져옵니다\n",
    "\n",
    "            # 얼굴과 감정 분석\n",
    "            for i in range(len(faces)):\n",
    "                x = recognition_preprocessing([faces[i]])\n",
    "                y_1 = model_1.predict(x)\n",
    "                y_2 = model_2.predict(x)\n",
    "                l = np.argmax(y_1 + y_2, axis=1)\n",
    "\n",
    "                # 각 얼굴에 대한 감정 카운트 증가\n",
    "                emotion_counts[l[0]] += 1\n",
    "\n",
    "            # 결과만 저장 (동영상 출력 없음)\n",
    "            save_emotion_results(video_path, emotion_counts)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # 통합된 딕셔너리에 결과 저장\n",
    "    combined_results[os.path.basename(video_path)] = emotion_counts\n",
    "\n",
    "# 통합된 결과를 하나의 JSON 파일로 저장\n",
    "combined_json_file_path = os.path.join(video_folder_path, 'combined_emotion_results.json')\n",
    "with open(combined_json_file_path, 'w') as json_file:\n",
    "    json.dump(combined_results, json_file, indent=4)\n",
    "\n",
    "print(f\"통합된 감정 분석 결과가 {combined_json_file_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 출력 없이 결과값만 저장 / 3초에 1번씩 프레임 처리\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "emotions = {\n",
    "    0: ['Angry', (0, 0, 255), (255, 255, 255)],\n",
    "    1: ['Disgust', (0, 102, 0), (255, 255, 255)],\n",
    "    2: ['Fear', (255, 255, 153), (0, 51, 51)],\n",
    "    3: ['Happy', (153, 0, 153), (255, 255, 255)],\n",
    "    4: ['Sad', (255, 0, 0), (255, 255, 255)],\n",
    "    5: ['Surprise', (0, 255, 0), (255, 255, 255)],\n",
    "    6: ['Neutral', (160, 160, 160), (255, 255, 255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'\n",
    "\n",
    "\n",
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1. / 255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "\n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                     loss=categorical_crossentropy,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_, h_max))\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48, 48))\n",
    "\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x\n",
    "\n",
    "\n",
    "def inference(image):\n",
    "\n",
    "    global emotion_counts\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "\n",
    "    if results.detections:\n",
    "\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2, x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            face = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "            face = tf.image.resize(face, (48, 48))\n",
    "\n",
    "            #밖에서 같은 행동을 또 하고 있기 때문에 안쪽 for문에 넣어서 한번에 처리\n",
    "            x = recognition_preprocessing(face)\n",
    "            y_1 = model_1.predict(x)\n",
    "            y_2 = model_2.predict(x)\n",
    "            l = np.argmax(y_1 + y_2, axis=1)\n",
    "            emotion_counts[l[0]] += 1\n",
    "\n",
    "\n",
    "# Video 감정분석\n",
    "def save_emotion_results(video_path, emotion_counts):\n",
    "    # 각 동영상에 대한 감정 분석 결과를 저장할 딕셔너리 생성\n",
    "    emotion_results = {\n",
    "        \"emotion_counts\": emotion_counts,\n",
    "        \"emotion_names\": [emotions[label][0] for label in emotion_counts.keys()],\n",
    "        \"emotion_values\": list(emotion_counts.values())\n",
    "    }\n",
    "\n",
    "    # 개별 동영상 결과를 저장\n",
    "    json_file_path = f'emotion_results_{os.path.basename(video_path)}.json'\n",
    "    json_file_path = os.path.join(video_folder_path, json_file_path)\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(emotion_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"{os.path.basename(video_path)}의 감정 분석 결과가 {json_file_path}에 저장되었습니다.\")\n",
    "\n",
    "    return emotion_results\n",
    "\n",
    "\n",
    "# 모든 동영상 결과를 저장할 빈 딕셔너리 초기화\n",
    "combined_results = {}\n",
    "\n",
    "# 동영상 파일이 있는 폴더 경로\n",
    "video_folder_path = 'C:/Users/user/Downloads/test'\n",
    "\n",
    "# 폴더 내의 모든 동영상 파일 경로를 얻어옴\n",
    "video_paths = glob.glob(os.path.join(video_folder_path, '*.mp4'))\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Load the video duration for accurate interval processing\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    video_duration = total_frames / fps\n",
    "\n",
    "    # 감정별 카운트 초기화 (각 프레임당 한 번 초기화)\n",
    "    emotion_counts = {emotion_label: 0 for emotion_label in emotions}\n",
    "\n",
    "    # 3초 카운터\n",
    "    idx = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "\n",
    "        # 3초당 1번씩 처리 (간격을 더 짧게 조정하여 자주 감정 분석)\n",
    "        # 초 처리\n",
    "        seconds = np.floor(cap.get(cv2.CAP_PROP_POS_MSEC)/ 1000) * 1000 - idx\n",
    "        if seconds % 3 == 0:\n",
    "            idx += 3\n",
    "            # 'inference' 이미지 가정 추정\n",
    "            inference(image)  # 'inference' 함수에서 얼굴을 가져옵니다\n",
    "\n",
    "            # 초기화 및 최종 결과 저장\n",
    "\n",
    "            # if elapsed_time >= 3.0:\n",
    "            save_emotion_results(video_path, emotion_counts)\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # 통합된 딕셔너리에 결과 저장\n",
    "    combined_results[os.path.basename(video_path)] = emotion_counts\n",
    "\n",
    "# 통합된 결과를 하나의 JSON 파일로 저장\n",
    "combined_json_file_path = os.path.join(video_folder_path, 'combined_emotion_results.json')\n",
    "with open(combined_json_file_path, 'w') as json_file:\n",
    "    json.dump(combined_results, json_file, indent=4)\n",
    "\n",
    "print(f\"통합된 감정 분석 결과가 {combined_json_file_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정의 수정 - 최종\n",
    "# 영상 출력 없이 결과값만 저장 / 3초에 1번씩 프레임 처리\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "emotions = {\n",
    "    0: ['Angry', (0, 0, 255), (255, 255, 255)],\n",
    "    1: ['Disgust', (0, 102, 0), (255, 255, 255)],\n",
    "    2: ['Fear', (255, 255, 153), (0, 51, 51)],\n",
    "    3: ['Happy', (153, 0, 153), (255, 255, 255)],\n",
    "    4: ['Sad', (255, 0, 0), (255, 255, 255)],\n",
    "    5: ['Surprise', (0, 255, 0), (255, 255, 255)],\n",
    "    6: ['Neutral', (160, 160, 160), (255, 255, 255)]\n",
    "}\n",
    "num_classes = len(emotions)\n",
    "input_shape = (48, 48, 1)\n",
    "weights_1 = 'vggnet.h5'\n",
    "weights_2 = 'vggnet_up.h5'\n",
    "\n",
    "\n",
    "class VGGNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.add(Rescaling(1. / 255, input_shape=input_shape))\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.5))\n",
    "\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(MaxPool2D())\n",
    "        self.add(Dropout(0.4))\n",
    "\n",
    "        self.add(Flatten())\n",
    "\n",
    "        self.add(Dense(1024, activation='relu'))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(256, activation='relu'))\n",
    "\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer=Adam(learning_rate=lr),\n",
    "                     loss=categorical_crossentropy,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
    "model_1.load_weights(model_1.checkpoint_path)\n",
    "\n",
    "model_2 = VGGNet(input_shape, num_classes, weights_2)\n",
    "model_2.load_weights(model_2.checkpoint_path)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def detection_preprocessing(image, h_max=360):\n",
    "    h, w, _ = image.shape\n",
    "    if h > h_max:\n",
    "        ratio = h_max / h\n",
    "        w_ = int(w * ratio)\n",
    "        image = cv2.resize(image, (w_, h_max))\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_face(face):\n",
    "    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "    return tf.image.resize(x, (48, 48))\n",
    "\n",
    "\n",
    "def recognition_preprocessing(faces):\n",
    "    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n",
    "    return x\n",
    "\n",
    "\n",
    "def inference(image):\n",
    "\n",
    "    global emotion_counts\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "\n",
    "    # 안쓰니까 필요없다.\n",
    "    # faces = []  # 빈 리스트로 초기화\n",
    "\n",
    "    if results.detections:\n",
    "        # 그림을 안그리니 필요없다.\n",
    "        # pos = []\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x = int(box.xmin * W)\n",
    "            y = int(box.ymin * H)\n",
    "            w = int(box.width * W)\n",
    "            h = int(box.height * H)\n",
    "\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(x + w, W)\n",
    "            y2 = min(y + h, H)\n",
    "\n",
    "            face = image[y1:y2, x1:x2]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # face = face.expand_dims(0)\n",
    "            face = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n",
    "            face = tf.image.resize(face, (48, 48))\n",
    "\n",
    "            #밖에서 같은 행동을 또 하고 있기 때문에 안쪽 for문에 넣어서 한번에 처리\n",
    "            x = recognition_preprocessing(face)\n",
    "            y_1 = model_1.predict(x)\n",
    "            y_2 = model_2.predict(x)\n",
    "            l = np.argmax(y_1 + y_2, axis=1)\n",
    "            emotion_counts[l[0]] += 1\n",
    "\n",
    "            # 밖에서 페이스를 하나씩 풀기때문에 굳이 묶지 않는다.\n",
    "            # faces.append(face)\n",
    "            # 포즈 안쓴다.\n",
    "            # pos.append((x1, y1, x2, y2))           \n",
    "        \n",
    "\n",
    "        # 그림을 그리는 부분이 없다. => 그림 결과를 전혀 사용하지 않음.\n",
    "        # for i in range(len(faces)):\n",
    "        #     cv2.rectangle(image, (pos[i][0], pos[i][1]),\n",
    "        #                   (pos[i][2], pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "        #     cv2.rectangle(image, (pos[i][0], pos[i][1] - 20),\n",
    "        #                   (pos[i][2] + 20, pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "        #     cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0], pos[i][1] - 5),\n",
    "        #                 0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "# Video 감정분석\n",
    "def save_emotion_results(video_path, emotion_counts):\n",
    "    # 각 동영상에 대한 감정 분석 결과를 저장할 딕셔너리 생성\n",
    "    emotion_results = {\n",
    "        \"emotion_counts\": emotion_counts,\n",
    "        \"emotion_names\": [emotions[label][0] for label in emotion_counts.keys()],\n",
    "        \"emotion_values\": list(emotion_counts.values())\n",
    "    }\n",
    "\n",
    "    # 개별 동영상 결과를 저장\n",
    "    json_file_path = f'emotion_results_{os.path.basename(video_path)}.json'\n",
    "    json_file_path = os.path.join(video_folder_path, json_file_path)\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(emotion_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"{os.path.basename(video_path)}의 감정 분석 결과가 {json_file_path}에 저장되었습니다.\")\n",
    "\n",
    "    return emotion_results\n",
    "\n",
    "\n",
    "# 모든 동영상 결과를 저장할 빈 딕셔너리 초기화\n",
    "combined_results = {}\n",
    "\n",
    "# 동영상 파일이 있는 폴더 경로\n",
    "video_folder_path = 'C:/Users/user/Downloads/test'\n",
    "\n",
    "# 폴더 내의 모든 동영상 파일 경로를 얻어옴\n",
    "video_paths = glob.glob(os.path.join(video_folder_path, '*.mp4'))\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Load the video duration for accurate interval processing\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    video_duration = total_frames / fps\n",
    "\n",
    "    # 감정별 카운트 초기화 (각 프레임당 한 번 초기화)\n",
    "    emotion_counts = {emotion_label: 0 for emotion_label in emotions}\n",
    "\n",
    "    # 3초 카운터\n",
    "    idx = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "\n",
    "        # 3초당 1번씩 처리 (간격을 더 짧게 조정하여 자주 감정 분석)\n",
    "        # 초 처리\n",
    "        seconds = np.floor(cap.get(cv2.CAP_PROP_POS_MSEC)/ 1000) * 1000 - idx\n",
    "        if seconds % 3 == 0:\n",
    "            idx += 3\n",
    "            # 'inference' 이미지 가정 추정\n",
    "            inference(image)  # 'inference' 함수에서 얼굴을 가져옵니다\n",
    "\n",
    "            # # 얼굴과 감정 분석 => 인퍼런스 함수에서 이미 했다.\n",
    "            # for i in range(len(faces)):\n",
    "            #     x = recognition_preprocessing([faces[i]])\n",
    "            #     y_1 = model_1.predict(x)\n",
    "            #     y_2 = model_2.predict(x)\n",
    "            #     l = np.argmax(y_1 + y_2, axis=1) =>왜 매번 앙상블/>\n",
    "\n",
    "                # 각 얼굴에 대한 감정 카운트 증가\n",
    "            \n",
    "\n",
    "            # 초기화 및 최종 결과 저장\n",
    "            # 결과 저장하는 시간이 처리하는 시간이랑 똑같다.\n",
    "            # if elapsed_time >= 3.0:\n",
    "            save_emotion_results(video_path, emotion_counts)\n",
    "            start_time = time.time()\n",
    "            # 3초 간격으로 최종 결과만 저장하고 나면 더 이상 처리할 필요 없음\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # 통합된 딕셔너리에 결과 저장\n",
    "    combined_results[os.path.basename(video_path)] = emotion_counts\n",
    "\n",
    "# 통합된 결과를 하나의 JSON 파일로 저장\n",
    "combined_json_file_path = os.path.join(video_folder_path, 'combined_emotion_results.json')\n",
    "with open(combined_json_file_path, 'w') as json_file:\n",
    "    json.dump(combined_results, json_file, indent=4)\n",
    "\n",
    "print(f\"통합된 감정 분석 결과가 {combined_json_file_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
